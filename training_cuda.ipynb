{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nd7atMvdU278"
      },
      "outputs": [],
      "source": [
        "!pip install import-ipynb\n",
        "import import_ipynb\n",
        "#make sure to have Utils in the Colab file\n",
        "import Utils"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, time, pickle\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt"
      ],
      "metadata": {
        "id": "cVnWm1qTVBzx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model( G, D, loss_key, training_data, root,model, disc, epochnum = 200, size=256,order = True,crop= True,):\n",
        "  #loss_keys:\n",
        "  #-5: only L1, just used for testing in the end, reverse oreder\n",
        "  #0: only BCE_loss\n",
        "  #1: BCE +L1, L1 small\n",
        "  #2: BCE +L1, L1 big\n",
        "  #3: BCE+ MSE(L2), L2 small\n",
        "  #4: BCE+ MSE, Le big\n",
        "\n",
        "\n",
        "  # loss\n",
        "  BCE_loss = nn.BCELoss().cuda()\n",
        "  L1_loss = nn.L1Loss().cuda()\n",
        "  MSE_loss = nn.MSELoss().cuda()\n",
        "\n",
        "  # Adam optimizer\n",
        "  G_optimizer = optim.Adam(G.parameters(), lr=0.002, betas=(0.5, 0.99))\n",
        "  D_optimizer = optim.Adam(D.parameters(), lr=0.002, betas=(0.5, 0.99))\n",
        "\n",
        "  train_hist = {}\n",
        "  train_hist['D_losses'] = []\n",
        "  train_hist['G_losses'] = []\n",
        "  train_hist['per_epoch_ptimes'] = []\n",
        "  train_hist['total_ptime'] = []\n",
        "\n",
        "  print('training start!')\n",
        "  start_time = time.time()\n",
        "  for epoch in range(epochnum):\n",
        "      D_losses = []\n",
        "      G_losses = []\n",
        "      epoch_start_time = time.time()\n",
        "      num_iter = 0\n",
        "      for x_, _ in training_data:\n",
        "          # train discriminator D\n",
        "          D.zero_grad()\n",
        "       \n",
        "          x_,y_ = Utils.prepare_image(x_,size=256,order = order,crop= crop)\n",
        "          x_, y_ = Variable(x_.cuda()), Variable(y_.cuda())\n",
        "\n",
        "          D_result = D(x_, y_).squeeze()\n",
        "          #print(D_result)\n",
        "          \n",
        "          D_real_loss = BCE_loss(D_result, Variable(torch.ones(D_result.size()).cuda()))\n",
        "\n",
        "          G_result = G(x_)\n",
        "  \n",
        "          D_result = D(x_, G_result).squeeze()\n",
        "          #print(D_result)\n",
        "          \n",
        "          D_fake_loss = BCE_loss(D_result, Variable(torch.zeros(D_result.size()).cuda()))\n",
        "\n",
        "          D_train_loss = (D_real_loss + D_fake_loss) * 0.5\n",
        "          D_train_loss.backward()\n",
        "          D_optimizer.step()\n",
        "\n",
        "          train_hist['D_losses'].append(D_train_loss.data.cpu())\n",
        "\n",
        "          D_losses.append(D_train_loss.data)\n",
        "\n",
        "          # train generator G\n",
        "          G.zero_grad()\n",
        "\n",
        "          G_result = G(x_)\n",
        "          D_result = D(x_, G_result).squeeze()\n",
        "\n",
        "          if loss_key == 0:\n",
        "            G_train_loss = BCE_loss(D_result, Variable(torch.ones(D_result.size()).cuda()))\n",
        "          elif loss_key == 1:\n",
        "            G_train_loss = BCE_loss(D_result, Variable(torch.ones(D_result.size()).cuda())) + 10 * L1_loss(G_result, y_)\n",
        "          elif loss_key == 2:\n",
        "            G_train_loss = BCE_loss(D_result, Variable(torch.ones(D_result.size()).cuda())) + 500 * L1_loss(G_result, y_)\n",
        "          elif loss_key == 3:\n",
        "            G_train_loss = BCE_loss(D_result, Variable(torch.ones(D_result.size()).cuda())) + 10 * MSE_loss(G_result, y_)\n",
        "          elif loss_key == 4:\n",
        "            G_train_loss = BCE_loss(D_result, Variable(torch.ones(D_result.size()).cuda())) + 500 * MSE_loss(G_result, y_)\n",
        "          elif loss_key == -5:\n",
        "            G_train_loss = L1_loss(G_result, y_)\n",
        "          else:\n",
        "            G_train_loss = 0\n",
        "          G_train_loss.backward()\n",
        "          G_optimizer.step()\n",
        "          train_hist['G_losses'].append(G_train_loss.data.cpu())\n",
        "          G_losses.append(G_train_loss.data)\n",
        "\n",
        "          num_iter += 1\n",
        "          if num_iter % 50 == 0:\n",
        "            print(num_iter)\n",
        "          if num_iter == 1:\n",
        "            fixed_x_ = x_\n",
        "            fixed_y_ = y_\n",
        "\n",
        "      epoch_end_time = time.time()\n",
        "      per_epoch_ptime = epoch_end_time - epoch_start_time\n",
        "\n",
        "      print('epoch ', epoch+1, ' finished')\n",
        "      if disc == 70:\n",
        "        fixed_p = root + 'Fixed_results/loss_key'+str(loss_key)+'/' + model  + str(epoch+1) + '.png'\n",
        "      else:\n",
        "        fixed_p = root + 'Fixed_results/loss_key'+str(loss_key)+'_GAN'+str(disc)+'/' + model  + str(epoch + 1) + '.png'\n",
        "      Utils.create_picture(G,Variable(fixed_x_.cuda(), volatile=True), fixed_y_, (epoch+1),fixed_p)\n",
        "      train_hist['per_epoch_ptimes'].append(per_epoch_ptime)\n",
        "\n",
        "  end_time = time.time()\n",
        "  total_ptime = end_time - start_time\n",
        "  train_hist['total_ptime'].append(total_ptime)\n",
        "\n",
        "  #print(\"Avg one epoch ptime: %.2f, total %d epochs ptime: %.2f\" % (torch.mean(torch.FloatTensor(train_hist['per_epoch_ptimes'])), total_ptime))\n",
        "  print(\"Training finish!... save training results\")\n",
        "  if disc == 70:\n",
        "    torch.save(G.state_dict(), root + model+ str(loss_key) + 'generator_param.pkl')\n",
        "    torch.save(D.state_dict(), root + model+ str(loss_key) + 'discriminator_param.pkl')\n",
        "    with open(root + model +str(loss_key)+ 'train_hist.pkl', 'wb') as f:\n",
        "      pickle.dump(train_hist, f)\n",
        "      Utils.plot_results(train_hist, root, model, epoch, loss_key) \n",
        "  else:\n",
        "    torch.save(G.state_dict(), root + model+ str(loss_key)+'GAN'+str(disc) + 'generator_param.pkl')\n",
        "    torch.save(D.state_dict(), root + model+ str(loss_key) +'GAN'+str(disc) + 'discriminator_param.pkl')\n",
        "    with open(root + model +str(loss_key)+'GAN'+str(disc) + 'train_hist.pkl', 'wb') as f:\n",
        "      pickle.dump(train_hist, f)\n",
        "    \n",
        "  print('results_saved') "
      ],
      "metadata": {
        "id": "tgN7jzNzVNDT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_images(dataset, generator, test_data,key, order= True):\n",
        "  n=1\n",
        "  for x_, _ in test_data:\n",
        "    size = x_.size()[2] \n",
        "    if order:\n",
        "      y_ = x_[:, :, :, 0:size]\n",
        "      x_ = x_[:, :, :, size:]\n",
        "    else:\n",
        "      y_ = x_[:, :, :, size:]\n",
        "      x_ = x_[:, :, :, 0:size]\n",
        "\n",
        "    test_image = generator(x_)\n",
        "    path = dataset + '_results/test_results/loss_key'+str(key)+'/' + str(n) + '_input.png'\n",
        "    plt.imsave(path, (x_[0].cpu().data.numpy().transpose(1, 2, 0) + 1) / 2)\n",
        "    path = dataset + '_results/test_results/loss_key'+str(key) +'/' +str(n) + '_output.png'\n",
        "    plt.imsave(path, (test_image[0].cpu().data.numpy().transpose(1, 2, 0) + 1) / 2)\n",
        "    path =dataset + '_results/test_results/loss_key' +str(key)+'/'+ str(n) + '_target.png'\n",
        "    \n",
        "    plt.imsave(path, (y_[0].numpy().transpose(1, 2, 0) + 1) / 2)\n",
        "    n += 1\n"
      ],
      "metadata": {
        "id": "7SqN3DTKVRnR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_models_disc(generator, discriminator, test_load, order = True):\n",
        "  size = 256\n",
        "  n= 0\n",
        "  gen_loss = 0\n",
        "  for x_,_ in test_load:\n",
        "    if order:\n",
        "      x_ = x_[:, :, :, size:]\n",
        "    else:\n",
        "      x_ = x_[:, :, :, 0:size]\n",
        "    y_ = generator(x_)\n",
        "    lossx = discriminator(x_,y_).detach().numpy()\n",
        "    lossxm = np.mean(lossx)\n",
        "    gen_loss += lossxm\n",
        "    n += 1\n",
        "    \n",
        "\n",
        "  return gen_loss/n"
      ],
      "metadata": {
        "id": "8dVmaVBcVVOH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_models_inv(generator_1, generator_2, test_loader, order = True):\n",
        "  size = 256\n",
        "  L1_loss = nn.L1Loss()\n",
        "  n= 0\n",
        "  gen_loss = 0\n",
        "  for x_,_ in test_loader:\n",
        "    if order:\n",
        "      x_ = x_[:, :, :, size:]\n",
        "    else:\n",
        "      x_ = x_[:, :, :, 0:size]\n",
        "    y_ = generator_1(x_)\n",
        "    xn = generator_2(y_)\n",
        "    lossxt = L1_loss(x_,xn)\n",
        "    gen_loss += lossxt\n",
        "    n += 1\n",
        "  loss = gen_loss.item()\n",
        "\n",
        "  return loss/n"
      ],
      "metadata": {
        "id": "CQlsc1BxVY_v"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}